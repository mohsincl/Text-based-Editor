{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class essayGrader(object):\n",
    "    \"\"\"\n",
    "    Contains methods to create features\n",
    "    responsible for producing grading\n",
    "    for the text essay\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dataframe):\n",
    "        \"\"\"\n",
    "        Constructor which takes in input as\n",
    "        the datafram containing default features\n",
    "        of the essay (if present)\n",
    "        \"\"\"\n",
    "        self.df = dataframe\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns information about essay\n",
    "        dataframe\n",
    "        \"\"\"\n",
    "        return str(self.df.info())\n",
    "    \n",
    "    def spell_check(self):\n",
    "        \"\"\"\n",
    "        Spellings are checked and scores\n",
    "        are provided for the given essay.\n",
    "        Here scores are normalised by the\n",
    "        length of the essay.\n",
    "        \"\"\"\n",
    "        print 'Run SpellCheck.py'\n",
    "    \n",
    "    def get_complexity(self,col):\n",
    "        \"\"\"\n",
    "        Takes input as the column of request and calculate\n",
    "        the complexity score given by Flesch-Kincaid Grade \n",
    "        Level.\n",
    "        Returns a list of complexity score for each request.\n",
    "        \"\"\"\n",
    "        grade_level = []\n",
    "        syl = set(['a','e','i','o','u'])\n",
    "        for text in self.df[col]:\n",
    "            sent_cnt = len(text.split('.'))\n",
    "            words = re.sub(\"[\"+'!\"#$%&\\'()*+.,-/:;<=>?@[\\\\]^_`{|}~'+\"]\", \" \", text).split()\n",
    "            syl_count = 0\n",
    "            for word in words:\n",
    "                for letter in list(word):\n",
    "                    if letter in syl:\n",
    "                        syl_count += 1\n",
    "            grade_level.append(Flesch_reading_ease(total_sentences = sent_cnt,total_words = len(words),\\\n",
    "                                                   total_sylabls=syl_count))\n",
    "        return grade_level\n",
    "    \n",
    "    def get_length(self,col):\n",
    "        f = lambda x : len(x.split())\n",
    "        length = pd.Series([f(x) for x in self.df[col]])\n",
    "        return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3 \n",
    "def train_model(sentences,num_features=300,min_word_count=40,num_workers=4,context=10,downsampling=1e-3):\n",
    "    # Set values for various parameters\n",
    "    num_features = 300    # Word vector dimensionality                      \n",
    "    min_word_count = 40   # Minimum word count                        \n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 10          # Context window size                                                                                    \n",
    "    downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "    from gensim.models import word2vec\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"300features_40minwords_10context\"\n",
    "    model.save(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12976, 28)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('Data/training_set_rel3.tsv',delimiter='\\t')\n",
    "print df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12976 entries, 0 to 12975\n",
      "Data columns (total 28 columns):\n",
      "essay_id          12976 non-null int64\n",
      "essay_set         12976 non-null int64\n",
      "essay             12976 non-null object\n",
      "rater1_domain1    12976 non-null int64\n",
      "rater2_domain1    12976 non-null int64\n",
      "rater3_domain1    128 non-null float64\n",
      "domain1_score     12976 non-null int64\n",
      "rater1_domain2    1800 non-null float64\n",
      "rater2_domain2    1800 non-null float64\n",
      "domain2_score     1800 non-null float64\n",
      "rater1_trait1     2292 non-null float64\n",
      "rater1_trait2     2292 non-null float64\n",
      "rater1_trait3     2292 non-null float64\n",
      "rater1_trait4     2292 non-null float64\n",
      "rater1_trait5     723 non-null float64\n",
      "rater1_trait6     723 non-null float64\n",
      "rater2_trait1     2292 non-null float64\n",
      "rater2_trait2     2292 non-null float64\n",
      "rater2_trait3     2292 non-null float64\n",
      "rater2_trait4     2292 non-null float64\n",
      "rater2_trait5     723 non-null float64\n",
      "rater2_trait6     723 non-null float64\n",
      "rater3_trait1     128 non-null float64\n",
      "rater3_trait2     128 non-null float64\n",
      "rater3_trait3     128 non-null float64\n",
      "rater3_trait4     128 non-null float64\n",
      "rater3_trait5     128 non-null float64\n",
      "rater3_trait6     128 non-null float64\n",
      "dtypes: float64(22), int64(5), object(1)\n",
      "memory usage: 2.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "essay = essayGrader(df_train)\n",
    "print essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting complexity of the essay...\n",
      "12976\n"
     ]
    }
   ],
   "source": [
    "essay_col = 'essay'\n",
    "print 'Getting complexity of the essay...'\n",
    "complexity = essay.get_complexity(essay_col)\n",
    "print len(complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_to_wordlist(sentence,remove_stopwords=True):\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "    words = sentence.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_into_sents(text,remove_stopwords=True):\n",
    "    parsed_essay = []\n",
    "    text = unicode(text, errors='ignore')\n",
    "    sents = tokenizer.tokenize(text.strip().decode('utf-8'))\n",
    "    for sent in sents:\n",
    "        if len(sent) > 0:\n",
    "            parsed_essay.append(convert_to_wordlist(sent,remove_stopwords=remove_stopwords))\n",
    "    return parsed_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training data....\n"
     ]
    }
   ],
   "source": [
    "print 'Parsing sentences from training data....'\n",
    "sentences = []\n",
    "for text in df_train['essay']:\n",
    "    sentences += parse_into_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_feat_vector(words,model,num_features):\n",
    "    featVector = np.zeros((num_features,))\n",
    "    vocab = set(model.wv.index2word)\n",
    "    nwords = 0\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            featVector += model[word]\n",
    "            nwords += 1\n",
    "    return featVector/nwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_feat_matrix(df,col,model,num_features):\n",
    "    featMatrix,i = np.zeros((len(df[col]),num_features)),0\n",
    "    for essay in df[col]:\n",
    "        essay = unicode(essay, errors='ignore')\n",
    "        featVector = create_feat_vector(essay.strip().lower().split(),model,num_features)\n",
    "        featMatrix[i] = featVector\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print '{} reviews parsed...'.format(i)\n",
    "    return featMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Train word2vec model\n",
    "model = train_model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 reviews parsed...\n",
      "2000 reviews parsed...\n",
      "3000 reviews parsed...\n",
      "4000 reviews parsed...\n",
      "5000 reviews parsed...\n",
      "6000 reviews parsed...\n",
      "7000 reviews parsed...\n",
      "8000 reviews parsed...\n",
      "9000 reviews parsed...\n",
      "10000 reviews parsed...\n",
      "11000 reviews parsed...\n",
      "12000 reviews parsed...\n"
     ]
    }
   ],
   "source": [
    "featMatrix = create_feat_matrix(df_train,'essay',model,num_features)\n",
    "featMatrix[11748] = create_feat_vector(df_train.iloc[11748,:].essay,model,num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(featMatrix,columns=['feature{}'.format(i) for i in range(num_features)])\n",
    "df_new['complexity'] = complexity\n",
    "df_new['spell_check'] = pd.read_csv('spellCheck.csv',header=None)\n",
    "df_new['length'] = essay.get_length(essay_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "y = df_train.rater1_domain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_new,y,random_state=7,test_size=0.1)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Error: {}'.format(np.sqrt(np.sum(np.square(rfc.predict(X_test) - y_test))))/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
